## ğŸ”¹ What Are Positional Embeddings?
Transformers donâ€™t read text in order automatically.  
So **positional embeddings** tell the model *where each token is located* in the sequence.

## ğŸ”¹ Why Do We Need Them?
Without position info, the model canâ€™t understand:
- word order  
- sequence meaning  
- relationships like â€œA â†’ B â†’ Câ€

Example:  
â€œDog bites manâ€ vs â€œMan bites dogâ€  
Same words, **different position â†’ different meaning**.

## ğŸ”¹ How They Work (Simple)
For every token, the model adds a **position vector**:

token_embedding + position_embedding = final_embedding

This helps the model know:
- token 1  
- token 2  
- token 3  
â€¦ etc.

## ğŸ”¹ Types of Positional Embeddings
1. **Learned positional embeddings**  
   - Model learns position weights during training  
   - Used in BERT

2. **Sinusoidal positional embeddings**  
   - Fixed pattern using sine/cosine functions  
   - Used in original Transformers  
   - Helps extrapolate to longer sequences

## ğŸ”¹ One-Line Summary
**Positional embeddings give tokens a sense of order so the Transformer can understand sequences correctly.**
