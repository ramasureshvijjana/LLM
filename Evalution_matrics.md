# 13.1 **BLEU - Bilingual Evaluation Understudy**

- To measure **how much generated text matches with one or more reference text docs**.
- BLEU is an **n-gram precision analysis** used to evaluate the text generated by LLM.

## **Value Limits**
**Range:** `0 to 1`

- **0 ‚Üí** No overlap between generated and reference text **(WORST case)**  
- **1 ‚Üí** Perfect match between generated and reference text **(BEST case)**

## **Mathematical Formula**

$$
\text{BLEU Score} = BP \times \exp\left( \sum_{i=1}^{n} w_i \log(p_i) \right)
$$

Where:
- \( BP \) = **Brevity Penalty**  
  ‚Üí Used to penalize **short sentences**  

### **Brevity Penalty (BP)**
$$
BP = 1 \quad \text{when generated length ‚â• reference length}
$$

$$
BP = \exp \left( 1 - \frac{\text{Reference length}}{\text{Generated length}} \right)
$$

---
# 13.2 ROUGE

- ROUGE score evaluates the **quality of generated text by comparing it to reference texts using overlap of *n-grams, word sequences, and word pairs***.

## ROUG Scores:

- **Precision:** Measures how much of the generated content is **relevant**.  
- **Recall:** Measures how much of the **reference content is captured by the generated text**.  
- **F1 Score:** Harmonic mean of precision and recall, **balancing both Precision and Recall for overall quality**.  

## ROUGE Variants:
1. ROUGE-N
2. ROUGE-L
3. ROUGE-W
4. ROUGE-S
5. ROUGE-Lsum

## ROUGE-N: 
- Measures n-gram overlap (e.g., ROUGE-1 for unigrams, ROUGE-2 for bigrams).

## üß† Example: ROUGE-1

**Reference Text**:  `The cat sat on the mat.` &emsp;&emsp; **Generated Text**:  `The cat lay on the rug.`

### Unigrams (words):
- Reference: `the`, `cat`, `sat`, `on`, `the`, `mat`
- Generated: `the`, `cat`, `lay`, `on`, `the`, `rug`

**Overlap:** `the`, `cat`, `on`, `the` ‚Üí 4 overlapping unigrams

### Metrics:
- **Precision** = Overlap / Generated = `4 / 6 = 0.67`  
- **Recall** = Overlap / Reference = `4 / 6 = 0.67`  
- **F1 Score** = Harmonic mean of precision and recall = `0.67`

---

# 13.3 Perplexity

## What is Perplexity  

Perplexity is an LLM evaluation metric that explains how confused a model is when predicting the next word in a sequence.

---
## Perplexity Values

- **Perplexity = 1** ‚Üí The LLM is perfectly confident and makes no mistakes. It predicts the correct next word with 100% certainty.  
  *Example: If you ask ‚ÄúThe sun rises in the ___‚Äù, it‚Äôs sure the answer is **east***.

- **Perplexity = 10** ‚Üí The LLM is 10 times more ‚Äúconfused.‚Äù It‚Äôs much less confident and spreads probability across many possible words.  
  *Example: It isn‚Äôt sure whether to say **east, morning, sky, world,** etc.*

üëâ **Lower perplexity = better, more certain predictions**  
üëâ **Higher perplexity = more confused, less accurate predictions**

---
## Maths in Perplexity

### üìå Step 1: Perplexity Formula  

For a sentence (sequence of words), perplexity (PP) is:

$$
PP(W) = \exp\left(-\frac{1}{N} \sum_{i=1}^N \log P(w_i | w_1,...,w_{i-1}) \right)
$$
Or, if using base 2 logarithm:
$$
PP(W) = 2^\left(-\frac{1}{N} \sum_{i=1}^N \log P(w_i | w_1,...,w_{i-1}) \right)
$$

where:
- **N** = number of words in the sentence  
- **P(wi | previous words)** = probability given by the model  



### üìå Step 2: Simple Example  

Suppose our sentence has **3 words**:  
**"the cat sleeps"**  

And the model assigns these probabilities:  

- P(the) = 0.5  
- P(cat | the) = 0.25  
- P(sleeps | the cat) = 0.125  

### Step 3: average log probability  

$$
\frac{1}{N} \sum_{i=1}^N \log_2 P(w_i)
$$

$$
= \frac{1}{3}\Big(\log_2 0.5 + \log_2 0.25 + \log_2 0.125 \Big)
$$

$$
= \frac{1}{3}(1 + 2 + 3) = \frac{6}{3} = 2
$$

### Step 5: Perplexity  

Perplexity is:

$$
PP = 2^2 = 4
$$

---



$$
BP = \exp \left( 1 - \frac{\text{Reference length}}{\text{Generated length}} \right)
$$

---

