# 13.3 Perplexity

## What is Perplexity  

Perplexity is an LLM evaluation metric that explains how confused a model is when predicting the next word in a sequence.

---
## Perplexity Values

- **Perplexity = 1** â†’ The LLM is perfectly confident and makes no mistakes. It predicts the correct next word with 100% certainty.  
  *Example: If you ask â€œThe sun rises in the ___â€, itâ€™s sure the answer is **east***.

- **Perplexity = 10** â†’ The LLM is 10 times more â€œconfused.â€ Itâ€™s much less confident and spreads probability across many possible words.  
  *Example: It isnâ€™t sure whether to say **east, morning, sky, world,** etc.*

ğŸ‘‰ **Lower perplexity = better, more certain predictions**  
ğŸ‘‰ **Higher perplexity = more confused, less accurate predictions**

---
## Maths in Perplexity

### ğŸ“Œ Step 1: Perplexity Formula  

For a sentence (sequence of words), perplexity (PP) is:

$$
PP(W) = \exp\left(-\frac{1}{N} \sum_{i=1}^N \log P(w_i | w_1,...,w_{i-1}) \right)
$$
Or, if using base 2 logarithm:
$$
PP(W) = 2^\left(-\frac{1}{N} \sum_{i=1}^N \log P(w_i | w_1,...,w_{i-1}) \right)
$$

where:
- **N** = number of words in the sentence  
- **P(wi | previous words)** = probability given by the model  



### ğŸ“Œ Step 2: Simple Example  

Suppose our sentence has **3 words**:  
**"the cat sleeps"**  

And the model assigns these probabilities:  

- P(the) = 0.5  
- P(cat | the) = 0.25  
- P(sleeps | the cat) = 0.125  

### Step 3: average log probability  

$$
\frac{1}{N} \sum_{i=1}^N \log_2 P(w_i)
$$

$$
= \frac{1}{3}\Big(\log_2 0.5 + \log_2 0.25 + \log_2 0.125 \Big)
$$

$$
= \frac{1}{3}(1 + 2 + 3) = \frac{6}{3} = 2
$$

### Step 5: Perplexity  

Perplexity is:

$$
PP = 2^2 = 4
$$

---

# **BLEU - Bilingual Evaluation Understudy**

- BLEU is an **n-gram precision analysis** used to evaluate the text generated by LLM.

## **Purpose**
To measure **how much generated text matches with one or more reference text docs**.

## **Value Limits**
**Range:** `0 to 1`

- **0 â†’** No overlap between generated and reference text **(WORST case)**  
- **1 â†’** Perfect match between generated and reference text **(BEST case)**

## **Mathematical Formula**

$$
\text{BLEU Score} = BP \times \exp\left( \sum_{i=1}^{n} w_i \log(p_i) \right)
$$

Where:
- \( BP \) = **Brevity Penalty**  
  â†’ Used to penalize **short sentences**  

## **Brevity Penalty (BP)**
$$
BP = 1 \quad \text{when generated length â‰¥ reference length}
$$

$$
BP = \exp \left( 1 - \frac{\text{Reference length}}{\text{Generated length}} \right)
$$

---

