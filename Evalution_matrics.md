# 13.1 **BLEU - Bilingual Evaluation Understudy**

- To measure **how much generated text matches with one or more reference text docs**.
- BLEU is an **n-gram precision analysis** used to evaluate the text generated by LLM.

## **Value Limits**
**Range:** `0 to 1`

- **0 â†’** No overlap between generated and reference text **(WORST case)**  
- **1 â†’** Perfect match between generated and reference text **(BEST case)**

## **Mathematical Formula**

$$
\text{BLEU Score} = BP \times \exp\left( \sum_{i=1}^{n} w_i \log(p_i) \right)
$$

Where:
- \( BP \) = **Brevity Penalty**  
  â†’ Used to penalize **short sentences**  

## **Brevity Penalty (BP)**
$$
BP = 1 \quad \text{when generated length â‰¥ reference length}
$$

$$
BP = \exp \left( 1 - \frac{\text{Reference length}}{\text{Generated length}} \right)
$$

## wáµ¢ Weights
-  **wáµ¢ = Weight for each n-gram**, Usually **equal weights** are used.  
  In standard BLEU-4:

  $$
  w_i = \frac{1}{4} \quad \text{for } i = 1,2,3,4
  $$

âœ” So, wâ‚ = wâ‚‚ = wâ‚ƒ = wâ‚„ = **0.25**

**Meaning:**   
  - Each n-gram precision contributes **equally (25%)** to the final BLEU score.

---
# 13.2 ROUGE - (Recall-Oriented Understudy for Gisting Evaluation)

- ROUGE score evaluates the **quality of generated text by comparing it to reference texts using overlap of *n-grams, word sequences, and word pairs***.

## ROUG Scores:

- **Precision:** Measures how much of the generated content is **relevant**.  
- **Recall:** Measures how much of the **reference content is captured by the generated text**.  
- **F1 Score:** Harmonic mean of precision and recall, **balancing both Precision and Recall for overall quality**.

$$
F = \frac{(1 + \beta^2) \times P \times R}{ \beta^2 \times P + R}
$$

where Î² = 1 (equal weight to precision & recall)

## ROUGE Variants:
1. ROUGE-N
2. ROUGE-L
3. ROUGE-W
4. [ROUGE-S](https://github.com/ramasureshvijjana/LLM/blob/master/Evalution_matrics.md#-rouge-s)
5. ROUGE-Lsum

## 13.2.1 ROUGE-N: 
- Measures n-gram overlap (e.g., ROUGE-1 for unigrams, ROUGE-2 for bigrams).

## ğŸ“˜ Example: ROUGE-1

**Reference Text**:  `The cat sat on the mat.` &emsp;&emsp; **Generated Text**:  `The cat lay on the rug.`

### Unigrams (words):
- Reference: `the`, `cat`, `sat`, `on`, `the`, `mat`
- Generated: `the`, `cat`, `lay`, `on`, `the`, `rug`

**Overlap:** `the`, `cat`, `on`, `the` â†’ 4 overlapping unigrams

### Metrics:
- **Precision** = Overlap / Generated = `4 / 6 = 0.67`  
- **Recall** = Overlap / Reference = `4 / 6 = 0.67`  
- **F1 Score** = Harmonic mean of precision and recall = `0.67`

## 13.2..2 ğŸ§  ROUGE-L:
âœ… **L - (Longest Common Subsequence)**  
âœ… measures how much the **generated text** overlaps with the **reference text** based on the **Longest Common Subsequence (LCS)**.  
âœ… It focuses on the **order of words** (not just matching words).  

## ğŸ“˜ Example

- **Reference (correct answer):**  `The cat sat on the mat`
**Generated (model output):** `The cat is sitting on the mat`

#### Step 1: Find Longest Common Subsequence (LCS)
- Longest sequence of words appearing in both sentences in the **same order**:

- **LCS =** â€œThe cat on the matâ€  â†’ LCS length = **4**

#### Step 2: Compute Precision and Recall

| Formula | Meaning | Calculation |
|----------|----------|-------------|
| Precision (P) = LCS / length of generated | How much of generated matches | 4 / 7 = **0.5714** |
| Recall (R) = LCS / length of reference | How much of reference is covered | 4 / 6 = **0.6667** |


#### Step 3: Compute F-measure (ROUGE-L Score)

$$
F = \frac{(1 + \beta^2) \times P \times R}{ \beta^2 \times P + R}
$$

where Î² = 1 (equal weight to precision & recall)

$$
F = \frac{2 \times 0.5714 \times 0.6667}{0.5714 + 0.6667} = 0.615
$$

âœ… **ROUGE-L = 0.615 (or 61.5%)**

## 13.2.3 ğŸ§  ROUGE-W

**ROUGE-W (Weighted Longest Common Subsequence)**  is a **improved version of ROUGE-L** 

âœ… **ROUGE-L** â†’ counts any common subsequence  
âœ… **ROUGE-W** â†’ rewards **longer continuous sequences** more strongly

## ğŸ“˜ Example

**Reference:** `The cat sat on the mat` ; **Generated:**  `The cat is sitting on the mat`

### Step 1: Find Common Subsequences

- â€œThe catâ€ âœ… consecutive match (2 words)
- â€œon the matâ€ âœ… consecutive match (3 words)

### Step 2: Weighted Longest Common Subsequence

- ROUGE-W uses a **weighting factor (usually w = 1.2 or 1.5)** to make **longer continuous matches more valuable**.

The LCS is **weighted** as:

$$
\text{Weighted LCS} = \sum (\text{segment length})^w
$$

If we use **w = 1.2**:

$$
LCS = 2^{1.2} + 3^{1.2} = 2.297 + 3.737 = 6.034
$$

### Step 3: Compute Recall and Precision

| Metric | Formula | Calculation |
|---------|----------|-------------|
| Precision (P) | LCS / Generated | 6.034 / 7 = **0.862** |
| Recall (R) | LCS / Reference | 6.034 / 6 = **1.0056** (â‰ˆ 1.0 cap at 1.0) |

### Step 4: Compute F-measure (ROUGE-W Score)

$$
F = \frac{2 \times P \times R}{P + R}
$$

$$
F = \frac{2 \times 0.862 \times 1.0}{0.862 + 1.0} = 0.925
$$

âœ… **ROUGE-W = 0.925 (or 92.5%)**

## 13.2.4 ğŸ§  ROUGE-S

- **ROUGE-S** measures how well the **order and co-occurrence of words** in the generated text matchs  the reference text by using **skip-bigrams**.  
- **Whatâ€™s a Skip-Bigram?** - **any pair of words that appear in the same order**, even if theyâ€™re not next to each other, is called **skip-bigram**.  
- It captures **word order** and **semantic similarity**, even if words in between skip-gram words.  

## ğŸ“˜ Example

**ğŸ§¾ Reference:**  `The cat sat on the mat` ;  **ğŸ¤– Generated:** `The cat is sitting on the mat`

### Step 1: Create Skip-Bigrams

- **Reference skip-bigrams:** (The, cat), (The, sat), (The, on), (The, the), (The, mat),
(cat, sat), (cat, on), (cat, the), (cat, mat),
(sat, on), (sat, the), (sat, mat),
(on, the), (on, mat),
(the, mat)  
&nbsp;âœ… **15 skip-bigrams**

- **Generated skip-bigrams:** (The, cat), (The, is), (The, sitting), (The, on), (The, the), (The, mat),
(cat, is), (cat, sitting), (cat, on), (cat, the), (cat, mat),
(is, sitting), (is, on), (is, the), (is, mat),
(sitting, on), (sitting, the), (sitting, mat),
(on, the), (on, mat),
(the, mat)  
&nbsp;âœ… **20 skip-bigrams**

- **Count Matches:** (The, cat), (The, on), (The, the), (The, mat),
(cat, on), (cat, the), (cat, mat),
(on, the), (on, mat), (the, mat)  
&nbsp;âœ… **10 matching skip-bigrams**

### Step 2: Compute Recall and Precision

| Formula |  Calculation |
|----------| -------------|
| Precision (P) = matching / total in generated | 10 / 20 = **0.5** |
| Recall (R) = matching / total in reference | 10 / 15 = **0.6667** |


### Step 4: Compute F-measure (ROUGE-S Score)

$$
F = \frac{2 \times P \times R}{P + R}
$$

$$
F = \frac{2 \times 0.5 \times 0.6667}{0.5 + 0.6667} = 0.5714
$$

âœ… **ROUGE-S = 0.571 (or 57.1%)**

## 13.2..5 ğŸ§ ROUGE-Lsum

- **ROUGE-Lsum** (ROUGE-L for Summaries)  
- It is a **sentence-level extension** of **ROUGE-L** â€” it measures how well the **generated summary** matches the **reference summary**,  by **averaging the ROUGE-L scores across all sentences**.

âœ… ROUGE-L â†’ Compares one text vs. one reference  
âœ… **ROUGE-Lsum â†’ Compares summaries sentence by sentence**

## ğŸ§© Why ROUGE-Lsum?

- Itâ€™s designed for **multi-sentence summaries** where:
- Sentences can appear in different order  
- Word overlap might occur across different parts  
- You want a fair, overall similarity score

So instead of treating the entire summary as one long string,  
ROUGE-Lsum compares **each sentence in the generated summary** with **each sentence in the reference**,  
and averages their **ROUGE-L** scores.

---

## ğŸ“˜ Example

**Reference summary:**
> 1ï¸âƒ£ The cat sat on the mat.  
> 2ï¸âƒ£ It was sunny outside.

**Generated summary:**
> 1ï¸âƒ£ The cat is sitting on the mat.  
> 2ï¸âƒ£ The weather was bright and sunny.

### Step 1: Compute ROUGE-L for each pair

**Sentence 1 comparison:**
- Ref: â€œThe cat sat on the matâ€  
- Gen: â€œThe cat is sitting on the matâ€  
â†’ ROUGE-L â‰ˆ **0.615** (from our earlier example)

**Sentence 2 comparison:**
- Ref: â€œIt was sunny outsideâ€  
- Gen: â€œThe weather was bright and sunnyâ€  
â†’ Common words: â€œwasâ€, â€œsunnyâ€  
â†’ LCS = 2 words  
â†’ Recall = 2/4 = 0.5  
â†’ Precision = 2/5 = 0.4  
â†’ F = (2 Ã— 0.5 Ã— 0.4) / (0.5 + 0.4) = **0.444**

---

### Step 2: Compute ROUGE-Lsum (average F1)

$$
\text{ROUGE-Lsum} = \frac{0.615 + 0.444}{2} = 0.5295
$$

âœ… **ROUGE-Lsum = 0.53 (or 53%)**

## ğŸ§© Summary

| Metric | Meaning | Value |
|---------|----------|-------|
| ROUGE-L (sentence 1) | First sentence match | 0.615 |
| ROUGE-L (sentence 2) | Second sentence match | 0.444 |
| **ROUGE-Lsum** | Average across sentences | **0.53** |

### âœ… Formula Summary
$$
ROUGE\text{-}L_{sum} = \frac{1}{N} \sum_{i=1}^{N} ROUGE\text{-}L(\text{Ref}_i, \text{Gen}_i)
$$
where **N** = number of sentences in the summary.

---

# 13.3 Perplexity

## What is Perplexity  

- Perplexity is an LLM evaluation metric that explains how confused a model is when predicting the next word in a sequence.

## Perplexity Values

- **Perplexity = 1** â†’ The LLM is perfectly confident and makes no mistakes. It predicts the correct next word with 100% certainty.  
  *Example: If you ask â€œThe sun rises in the ___â€, itâ€™s sure the answer is **east***.

- **Perplexity = 10** â†’ The LLM is 10 times more â€œconfused.â€ Itâ€™s much less confident and spreads probability across many possible words.  
  *Example: It isnâ€™t sure whether to say **east, morning, sky, world,** etc.*

ğŸ‘‰ **Lower perplexity = better, more certain predictions**  
ğŸ‘‰ **Higher perplexity = more confused, less accurate predictions**

## Maths in Perplexity

### ğŸ“Œ Step 1: Perplexity Formula  

For a sentence (sequence of words), perplexity (PP) is:

$$
PP(W) = \exp\left(-\frac{1}{N} \sum_{i=1}^N \log P(w_i | w_1,...,w_{i-1}) \right)
$$
Or, if using base 2 logarithm:
$$
PP(W) = 2^\left(-\frac{1}{N} \sum_{i=1}^N \log P(w_i | w_1,...,w_{i-1}) \right)
$$

where:
- **N** = number of words in the sentence  
- **P(wi | previous words)** = probability given by the model  



### ğŸ“Œ Step 2: Simple Example  

Suppose our sentence has **3 words**:  
**"the cat sleeps"**  

And the model assigns these probabilities:  

- P(the) = 0.5  
- P(cat | the) = 0.25  
- P(sleeps | the cat) = 0.125  

### Step 3: average log probability  

$$
\frac{1}{N} \sum_{i=1}^N \log_2 P(w_i)
$$

$$
= \frac{1}{3}\Big(\log_2 0.5 + \log_2 0.25 + \log_2 0.125 \Big)
$$

$$
= \frac{1}{3}(1 + 2 + 3) = \frac{6}{3} = 2
$$

### Step 5: Perplexity  

Perplexity is:

$$
PP = 2^2 = 4
$$

$$
BP = \exp \left( 1 - \frac{\text{Reference length}}{\text{Generated length}} \right)
$$

---

