# 13.1 **BLEU - Bilingual Evaluation Understudy**

- To measure **how much generated text matches with one or more reference text docs**.
- BLEU is an **n-gram precision analysis** used to evaluate the text generated by LLM.

## **Value Limits**
**Range:** `0 to 1`

- **0 ‚Üí** No overlap between generated and reference text **(WORST case)**  
- **1 ‚Üí** Perfect match between generated and reference text **(BEST case)**

## **Mathematical Formula**

$$
\text{BLEU Score} = BP \times \exp\left( \sum_{i=1}^{n} w_i \log(p_i) \right)
$$

Where:
- \( BP \) = **Brevity Penalty**  
  ‚Üí Used to penalize **short sentences**  

### **Brevity Penalty (BP)**
$$
BP = 1 \quad \text{when generated length ‚â• reference length}
$$

$$
BP = \exp \left( 1 - \frac{\text{Reference length}}{\text{Generated length}} \right)
$$

---
# 13.2 ROUGE - (Recall-Oriented Understudy for Gisting Evaluation)

- ROUGE score evaluates the **quality of generated text by comparing it to reference texts using overlap of *n-grams, word sequences, and word pairs***.

## ROUG Scores:

- **Precision:** Measures how much of the generated content is **relevant**.  
- **Recall:** Measures how much of the **reference content is captured by the generated text**.  
- **F1 Score:** Harmonic mean of precision and recall, **balancing both Precision and Recall for overall quality**.

$$
F = \frac{(1 + \beta^2) \times P \times R}{ \beta^2 \times P + R}
$$

where Œ≤ = 1 (equal weight to precision & recall)

## ROUGE Variants:
1. ROUGE-N
2. ROUGE-L
3. ROUGE-W
4. [ROUGE-S](https://github.com/ramasureshvijjana/LLM/blob/master/Evalution_matrics.md#-rouge-s)
5. ROUGE-Lsum

## ROUGE-N: 
- Measures n-gram overlap (e.g., ROUGE-1 for unigrams, ROUGE-2 for bigrams).

## üìò Example: ROUGE-1

**Reference Text**:  `The cat sat on the mat.` &emsp;&emsp; **Generated Text**:  `The cat lay on the rug.`

### Unigrams (words):
- Reference: `the`, `cat`, `sat`, `on`, `the`, `mat`
- Generated: `the`, `cat`, `lay`, `on`, `the`, `rug`

**Overlap:** `the`, `cat`, `on`, `the` ‚Üí 4 overlapping unigrams

### Metrics:
- **Precision** = Overlap / Generated = `4 / 6 = 0.67`  
- **Recall** = Overlap / Reference = `4 / 6 = 0.67`  
- **F1 Score** = Harmonic mean of precision and recall = `0.67`

## üß† ROUGE-L:
‚úÖ **L - (Longest Common Subsequence)**  
‚úÖ measures how much the **generated text** overlaps with the **reference text** based on the **Longest Common Subsequence (LCS)**.  
‚úÖ It focuses on the **order of words** (not just matching words).  

## üìò Example

- **Reference (correct answer):**  `The cat sat on the mat`
**Generated (model output):** `The cat is sitting on the mat`

#### Step 1: Find Longest Common Subsequence (LCS)
- Longest sequence of words appearing in both sentences in the **same order**:

- **LCS =** ‚ÄúThe cat on the mat‚Äù  ‚Üí LCS length = **4**

#### Step 2: Compute Precision and Recall

| Formula | Meaning | Calculation |
|----------|----------|-------------|
| Precision (P) = LCS / length of generated | How much of generated matches | 4 / 7 = **0.5714** |
| Recall (R) = LCS / length of reference | How much of reference is covered | 4 / 6 = **0.6667** |


#### Step 3: Compute F-measure (ROUGE-L Score)

$$
F = \frac{(1 + \beta^2) \times P \times R}{ \beta^2 \times P + R}
$$

where Œ≤ = 1 (equal weight to precision & recall)

$$
F = \frac{2 \times 0.5714 \times 0.6667}{0.5714 + 0.6667} = 0.615
$$

‚úÖ **ROUGE-L = 0.615 (or 61.5%)**

## üß† ROUGE-S?

üîπ **ROUGE-S** measures how well the **order and co-occurrence of words** in the generated text match the reference text by using something called **skip-bigrams**.  

üîπ **What‚Äôs a Skip-Bigram?** - **any pair of words that appear in the same order**, even if they‚Äôre not next to each other, is called **skip-bigram**.  

üîπ It captures **word order** and **semantic similarity**, even if words in between skip-gram words.  


## üìò Example

**üßæ Reference:**  `The cat sat on the mat` ;  **ü§ñ Generated:** `The cat is sitting on the mat`

### Step 1: Create Skip-Bigrams

**Reference skip-bigrams:** (The, cat), (The, sat), (The, on), (The, the), (The, mat),
(cat, sat), (cat, on), (cat, the), (cat, mat),
(sat, on), (sat, the), (sat, mat),
(on, the), (on, mat),
(the, mat)  
‚úÖ **15 skip-bigrams**

**Generated skip-bigrams:** (The, cat), (The, is), (The, sitting), (The, on), (The, the), (The, mat),
(cat, is), (cat, sitting), (cat, on), (cat, the), (cat, mat),
(is, sitting), (is, on), (is, the), (is, mat),
(sitting, on), (sitting, the), (sitting, mat),
(on, the), (on, mat),
(the, mat)  

‚úÖ **20 skip-bigrams**

**Count Matches:** (The, cat), (The, on), (The, the), (The, mat),
(cat, on), (cat, the), (cat, mat),
(on, the), (on, mat), (the, mat)  

‚úÖ **10 matching skip-bigrams**

### Step 2: Compute Recall and Precision

| Formula |  Calculation |
|----------| -------------|
| Precision (P) = matching / total in generated | 10 / 20 = **0.5** |
| Recall (R) = matching / total in reference | 10 / 15 = **0.6667** |


### Step 4: Compute F-measure (ROUGE-S Score)

$$
F = \frac{2 \times P \times R}{P + R}
$$

$$
F = \frac{2 \times 0.5 \times 0.6667}{0.5 + 0.6667} = 0.5714
$$

‚úÖ **ROUGE-S = 0.571 (or 57.1%)**

---

# 13.3 Perplexity

## What is Perplexity  

Perplexity is an LLM evaluation metric that explains how confused a model is when predicting the next word in a sequence.

---
## Perplexity Values

- **Perplexity = 1** ‚Üí The LLM is perfectly confident and makes no mistakes. It predicts the correct next word with 100% certainty.  
  *Example: If you ask ‚ÄúThe sun rises in the ___‚Äù, it‚Äôs sure the answer is **east***.

- **Perplexity = 10** ‚Üí The LLM is 10 times more ‚Äúconfused.‚Äù It‚Äôs much less confident and spreads probability across many possible words.  
  *Example: It isn‚Äôt sure whether to say **east, morning, sky, world,** etc.*

üëâ **Lower perplexity = better, more certain predictions**  
üëâ **Higher perplexity = more confused, less accurate predictions**

---
## Maths in Perplexity

### üìå Step 1: Perplexity Formula  

For a sentence (sequence of words), perplexity (PP) is:

$$
PP(W) = \exp\left(-\frac{1}{N} \sum_{i=1}^N \log P(w_i | w_1,...,w_{i-1}) \right)
$$
Or, if using base 2 logarithm:
$$
PP(W) = 2^\left(-\frac{1}{N} \sum_{i=1}^N \log P(w_i | w_1,...,w_{i-1}) \right)
$$

where:
- **N** = number of words in the sentence  
- **P(wi | previous words)** = probability given by the model  



### üìå Step 2: Simple Example  

Suppose our sentence has **3 words**:  
**"the cat sleeps"**  

And the model assigns these probabilities:  

- P(the) = 0.5  
- P(cat | the) = 0.25  
- P(sleeps | the cat) = 0.125  

### Step 3: average log probability  

$$
\frac{1}{N} \sum_{i=1}^N \log_2 P(w_i)
$$

$$
= \frac{1}{3}\Big(\log_2 0.5 + \log_2 0.25 + \log_2 0.125 \Big)
$$

$$
= \frac{1}{3}(1 + 2 + 3) = \frac{6}{3} = 2
$$

### Step 5: Perplexity  

Perplexity is:

$$
PP = 2^2 = 4
$$

---



$$
BP = \exp \left( 1 - \frac{\text{Reference length}}{\text{Generated length}} \right)
$$

---

